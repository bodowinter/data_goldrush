<style type="text/css">
     body {width: 650px;
          margin: 0 auto;}
     h2 {margin-top: 50px;}
     p {font-size: 15px;}
     pre {margin-bottom: 40px;}
     strong { color: rgb(22, 105, 147);
          font-family: 'DejaVu Sans Mono',
          'Droid Sans Mono', 'Lucida Console', Consolas, 
          Monaco, monospace;}
     a {color: #2E5C00;font-weight: bold;}
     a:hover {color: #2E5C00;
          text-decoration: none;
          font-weight: bold;}
     a:visited {color: #2E5C00;
          text-decoration: none;
          font-weight: bold;}
     pre .literal {color: rgb(22, 105, 147);}
     pre .identifier {color: rgb(22, 105, 147);}
     pre .string {color: rgb(22, 105, 147);}
     pre .keyword {color: rgb(22, 105, 147);}
     pre .number {color: rgb(184, 138, 0);}
     h1 {text-align: center;}
</style>

Scraping the Swadesh list from Wikipedia
========================================================

```{r echo=FALSE}
options(warn=-1)
```

## Description
In this little demo, I will scrape the Swadesh list from Wikipedia. Then we'll download the SUBTLEX_UK frequency data from the [Center for Reading Research webpage](http://crr.ugent.be/) and combine it with the Swadesh list. We will ask the simple question: Are the words on the Swadesh list more or less frequent than the average English word?

## Preliminaries
First, in case you don't have these already, we will need to install the following packages (you need to uncomment this line):    

```{r results="hide"}
# install.packages(c("RCurl","XML","stringr","openxlsx"))
```

Then, set your working directory. Mine is "data_goldrush" on my Desktop (you need to change your file path accordingly):

```{r results="hide"}
setwd("/Users/teeniematlock/Desktop/data_goldrush/")
```

If there is no data folder, create one and set your working directory to that folder:

```{r results="hide"}
if(!file.exists("data")){
     dir.create("data")
	}
setwd("data")
```

## Scraping a Wikipedia table
Now, let's go to Google and type in "Swadesh wiki". This ultimately led me to [this page](http://en.wiktionary.org/wiki/Appendix:Swadesh_lists). We will use the **getURL()** function from the **RCurl** package to download the page. The resulting object **swadesh** now contains the full html text of the webpage.

```{r}
library(RCurl)
swadesh <- getURL("http://en.wiktionary.org/wiki/Appendix:Swadesh_lists")
```

Being good data scientists, we are interested in reproducibility. So let's save the download date in an external text file.

```{r}
writeLines(text=as.character(Sys.time()),
     "Swadesh_download_time.txt")
```

Now, we can use the **XML** package to process the **swadesh** file. We will use the **readHTMLTable()** file to extract all tables from the html object. The function will return a list with all the tables.

```{r}
library(XML)
swadesh <- readHTMLTable(swadesh)
```

Let's look at what the structure of this object is using the **str()** function:

```{r, echo=T}
str(swadesh)
```

## Downloading SUBTLEX data
To understand this output, it also helps to look back at the [Wikipedia page](http://en.wiktionary.org/wiki/Appendix:Swadesh_lists). It returns a list of length two. The second list seems to contain the Swadesh lists for the different languages. So we can get the English words as follows:

```{r}
swadesh <- swadesh[[2]]$English
```

Let's look at this object:
```{r}
head(swadesh)
```
Notice that it's a bit messy. We will want to get rid of any additional information, such as "pl." or "sg." This means that we need to do some kind of string manipulation on this vector. Howver, to do this, we first need to convert the vector to a character vector using **as.character()**:

```{r}
swadesh <- as.character(swadesh)
```

We will circumvent all of these problems by just extracting gthe first word using a regular expression. We use the **str_extract()** function from the **stringr** package.
```{r}
library(stringr)
swadesh <- str_extract(swadesh,"[a-zA-Z]+")
```
Now, let's put this into a dataframe and look at our result:
```{r}
swadesh <- data.frame(Word=swadesh)     # we name the column "Word"
head(swadesh)
```

Wonderful, let's combine this with word frequency data! I will use the wonderful SUBTLEX word frequencies (highly recommended), for which I found the link on the  [Center for Reading Research webpage](http://crr.ugent.be/). So we download the file and then create a log of the download time:

```{r cache=T}
download.file(url="http://crr.ugent.be/papers/SUBTLEX-UK.txt",destfile="SUBTLEX-UK.txt",method="curl")
writeLines(text=as.character(Sys.time()),
     "SUBTLEX_download_time.txt")
```
We will load in the file and have a quick look at it. The file is tab-separated, by the way:
```{r cache=T}
SUBTL = read.table("SUBTLEX-UK.txt",sep="\t",quote="",header=T)
```
Let's check whether there are any missing frequency values:
```{r}
any(!is.na(SUBTL$FreqCount))
```
Yes! Let's exclude those and then log-transform the word frequency data (commonly done, generally a good idea):
```{r}
SUBTL = SUBTL[!is.na(SUBTL$FreqCount),]
SUBTL$LogFreq = log(SUBTL$FreqCount)
```
Now we can merge the frequency data and the Swadesh list:
```{r}
swadesh$LogFreq = SUBTL[match(swadesh$Word,SUBTL$Spelling),]$LogFreq
```

Finally, let's save our output:
```{r}
write.table(swadesh,"swadesh.csv",sep=",",row.names=F)
```


## Exploring the frequency data
Now, to explore the frequency of words on the Swadesh list, let's compare the mean Swadesh frequency to the overall distribution of frequencies in SUBTLEX:

```{r, fig.width=8, fig.height=6}
plot(density(SUBTL$LogFreq))
abline(v=mean(swadesh$LogFreq),col="darkred",lwd=3)
```

Finally, let us test whether the Swadesh words are significantly more frequent than the SUBTLEX words.
```{r}
t.test(swadesh$LogFreq,mu=mean(SUBTL$LogFreq))
```
Voila, they are!

<br><br><br><br><br><br>